#!/usr/bin/env python3
"""
CHAIR-M: Sensory Deprivation Hallucination Evaluation

Evaluates hallucination in model reasoning when sensory modalities are masked.
- Muted: Audio is masked, audio claims are hallucinations
- Blind: Visual is masked, visual claims are hallucinations
"""

import argparse
import json
import os
from pathlib import Path
from typing import List, Dict, Any, Optional
import pandas as pd
from tqdm import tqdm
from pydantic import BaseModel, Field
import openai
from openai import OpenAI


# ==================== Pydantic Schema for Structured Output ====================

class SentenceAnalysis(BaseModel):
    """Analysis of a single sentence."""
    text: str = Field(..., description="The sentence text")
    has_visual_claim: bool = Field(..., description="True if sentence contains positive visual claims")
    has_audio_claim: bool = Field(..., description="True if sentence contains positive audio claims")


class SensoryExtraction(BaseModel):
    """Structured extraction of sensory claims from reasoning text."""
    sentences: List[SentenceAnalysis] = Field(..., description="List of analyzed sentences")
    visual_claims_list: List[str] = Field(..., description="List of visual claims (atomic facts)")
    audio_claims_list: List[str] = Field(..., description="List of audio claims (atomic facts)")


# ==================== System Prompt ====================

SYSTEM_PROMPT = """You are a strict data parser for a multimodal model evaluation. Your goal is to analyze the "Reasoning Text" generated by an AI and extract sensory information into a structured JSON format.

### Extraction Rules

1. **Analyze Sentences**: Break the text into individual sentences.

2. **Extract Atomic Claims**: For each sentence, extract atomic sensory claims/facts.
   * **Visual Claims**: Descriptions of visible objects, actions, colors, text (OCR), or scene appearance.
   * **Audio Claims**: Descriptions of sounds, music, speech content, tone, volume, or ambient noise.

3. **Handle Negations**:
   * Statements like "I cannot hear anything" or "The screen is black" are **Negative Claims**.
   * DO NOT include Negative Claims in the "Audio Claims" or "Visual Claims" lists (as they represent faithful perception of absence).

4. **Handle Metaphors**: Ignore metaphorical phrases (e.g., "It sounds reasonable", "I see what you mean").

### Output Format

Return a JSON object with:
- sentences: List of sentences with boolean flags for visual/audio claims
- visual_claims_list: List of atomic visual claims (strings)
- audio_claims_list: List of atomic audio claims (strings)

IMPORTANT: Negative claims (statements of absence) should NOT be counted as claims."""


# ==================== LLM Processing ====================

class SensoryClaimsExtractor:
    """Extracts sensory claims from reasoning text using LLM."""
    
    def __init__(self, api_key: str, model: str = "gpt-4o-mini", base_url: Optional[str] = None):
        """
        Initialize extractor.
        
        Args:
            api_key: OpenAI API key (or compatible API key)
            model: Model name (e.g., "gpt-4o-mini", "Qwen/Qwen2.5-7B-Instruct")
            base_url: Optional base URL for API (for using local models)
        """
        self.client = OpenAI(api_key=api_key, base_url=base_url)
        self.model = model
    
    def extract(self, reasoning_text: str) -> SensoryExtraction:
        """
        Extract sensory claims from reasoning text.
        
        Args:
            reasoning_text: The model's reasoning text (<think> content)
            
        Returns:
            SensoryExtraction object with parsed claims
        """
        try:
            completion = self.client.beta.chat.completions.parse(
                model=self.model,
                messages=[
                    {"role": "system", "content": SYSTEM_PROMPT},
                    {"role": "user", "content": f"Please analyze this reasoning text:\n\n{reasoning_text}"}
                ],
                response_format=SensoryExtraction,
                temperature=0.0,
            )
            
            return completion.choices[0].message.parsed
        
        except Exception as e:
            print(f"Error during extraction: {e}")
            # Return empty extraction on error
            return SensoryExtraction(
                sentences=[],
                visual_claims_list=[],
                audio_claims_list=[]
            )


# ==================== Metric Calculation ====================

def calculate_chair_scores(parsed: SensoryExtraction, input_type: str) -> Dict[str, Any]:
    """
    Calculate CHAIR-M scores for a single sample.
    
    Args:
        parsed: Extracted sensory claims
        input_type: "mask_audio" (Muted) or "mask_visual" (Blind)
        
    Returns:
        Dictionary with scores and counts
    """
    # Count claims
    num_visual = len(parsed.visual_claims_list)
    num_audio = len(parsed.audio_claims_list)
    num_total_claims = num_visual + num_audio
    
    # Count sentences
    num_total_sentences = len(parsed.sentences)
    
    # Identify hallucinations based on input type
    if input_type == "mask_audio" or input_type == "Muted":
        # Audio is masked, any audio claim is a hallucination
        num_hallucinated_claims = num_audio
        num_hallucinated_sentences = sum(
            1 for sent in parsed.sentences if sent.has_audio_claim
        )
        hallucinated_modality = "audio"
        
    elif input_type == "mask_visual" or input_type == "Blind":
        # Visual is masked, any visual claim is a hallucination
        num_hallucinated_claims = num_visual
        num_hallucinated_sentences = sum(
            1 for sent in parsed.sentences if sent.has_visual_claim
        )
        hallucinated_modality = "visual"
        
    else:
        raise ValueError(f"Unknown input_type: {input_type}")
    
    # Compute scores (handle division by zero)
    chair_claim_score = (
        num_hallucinated_claims / num_total_claims 
        if num_total_claims > 0 else 0.0
    )
    chair_sentence_score = (
        num_hallucinated_sentences / num_total_sentences 
        if num_total_sentences > 0 else 0.0
    )
    
    return {
        "chair_claim_score": chair_claim_score,
        "chair_sentence_score": chair_sentence_score,
        "num_visual_claims": num_visual,
        "num_audio_claims": num_audio,
        "num_total_claims": num_total_claims,
        "num_hallucinated_claims": num_hallucinated_claims,
        "num_total_sentences": num_total_sentences,
        "num_hallucinated_sentences": num_hallucinated_sentences,
        "hallucinated_modality": hallucinated_modality,
    }


# ==================== Data Loading ====================

def load_inference_results(jsonl_path: str) -> List[Dict[str, Any]]:
    """Load inference results from JSONL file."""
    results = []
    with open(jsonl_path, 'r', encoding='utf-8') as f:
        for line in f:
            if line.strip():
                results.append(json.loads(line))
    return results


def infer_mask_type_from_path(jsonl_path: str) -> str:
    """Infer mask type from file path."""
    path_lower = jsonl_path.lower()
    if "mask_audio" in path_lower or "muted" in path_lower:
        return "mask_audio"
    elif "mask_visual" in path_lower or "blind" in path_lower:
        return "mask_visual"
    else:
        raise ValueError(
            f"Cannot infer mask type from path: {jsonl_path}\n"
            "Path should contain 'mask_audio' or 'mask_visual'"
        )


# ==================== Main Evaluation ====================

def evaluate_chair_m(
    jsonl_path: str,
    output_csv: str,
    api_key: str,
    model: str = "gpt-4o-mini",
    base_url: Optional[str] = None,
    mask_type: Optional[str] = None,
):
    """
    Evaluate CHAIR-M scores for a dataset.
    
    Args:
        jsonl_path: Path to inference results JSONL
        output_csv: Path to save per-sample results CSV
        api_key: OpenAI API key
        model: LLM model name
        base_url: Optional API base URL
        mask_type: "mask_audio" or "mask_visual" (auto-inferred if None)
    """
    # Load data
    print(f"Loading data from {jsonl_path}...")
    data = load_inference_results(jsonl_path)
    print(f"Loaded {len(data)} samples")
    
    # Infer mask type if not provided
    if mask_type is None:
        mask_type = infer_mask_type_from_path(jsonl_path)
    print(f"Mask type: {mask_type}")
    
    # Initialize extractor
    print(f"Initializing extractor with model: {model}")
    extractor = SensoryClaimsExtractor(api_key=api_key, model=model, base_url=base_url)
    
    # Process each sample
    results = []
    for item in tqdm(data, desc="Evaluating"):
        sample_id = item.get("name", "unknown")
        reasoning_text = item.get("think", "")
        
        # Skip if no reasoning text
        if not reasoning_text or not reasoning_text.strip():
            print(f"Warning: Sample {sample_id} has no reasoning text, skipping")
            continue
        
        # Extract claims
        parsed = extractor.extract(reasoning_text)
        
        # Calculate scores
        scores = calculate_chair_scores(parsed, mask_type)
        
        # Store result
        result = {
            "sample_id": sample_id,
            "mask_type": mask_type,
            **scores,
            "reasoning_length": len(reasoning_text),
        }
        results.append(result)
    
    # Save per-sample results
    df = pd.DataFrame(results)
    df.to_csv(output_csv, index=False)
    print(f"\nSaved per-sample results to {output_csv}")
    
    # Print summary statistics
    print("\n" + "="*80)
    print("CHAIR-M Evaluation Results")
    print("="*80)
    print(f"Dataset: {jsonl_path}")
    print(f"Mask Type: {mask_type}")
    print(f"Total Samples: {len(results)}")
    print(f"\nAverage CHAIR-M (Claim Level): {df['chair_claim_score'].mean():.4f} ± {df['chair_claim_score'].std():.4f}")
    print(f"Average CHAIR-M (Sentence Level): {df['chair_sentence_score'].mean():.4f} ± {df['chair_sentence_score'].std():.4f}")
    print(f"\nAverage Claims per Sample:")
    print(f"  Visual: {df['num_visual_claims'].mean():.2f}")
    print(f"  Audio: {df['num_audio_claims'].mean():.2f}")
    print(f"  Total: {df['num_total_claims'].mean():.2f}")
    print(f"  Hallucinated: {df['num_hallucinated_claims'].mean():.2f}")
    print(f"\nAverage Sentences per Sample:")
    print(f"  Total: {df['num_total_sentences'].mean():.2f}")
    print(f"  Hallucinated: {df['num_hallucinated_sentences'].mean():.2f}")
    print("="*80)
    
    return df


# ==================== CLI ====================

def main():
    parser = argparse.ArgumentParser(
        description="CHAIR-M: Evaluate sensory hallucination in masked modality reasoning"
    )
    parser.add_argument(
        "--jsonl-path",
        required=True,
        help="Path to inference results JSONL file"
    )
    parser.add_argument(
        "--output-csv",
        required=True,
        help="Path to save per-sample results CSV"
    )
    parser.add_argument(
        "--api-key",
        default=None,
        help="OpenAI API key (or set OPENAI_API_KEY env var)"
    )
    parser.add_argument(
        "--model",
        default="gpt-4o-mini",
        help="LLM model name (default: gpt-4o-mini)"
    )
    parser.add_argument(
        "--base-url",
        default=None,
        help="Optional API base URL (for using local models)"
    )
    parser.add_argument(
        "--mask-type",
        choices=["mask_audio", "mask_visual"],
        default=None,
        help="Type of masking (auto-inferred from path if not provided)"
    )
    
    args = parser.parse_args()
    
    # Get API key
    api_key = args.api_key or os.environ.get("OPENAI_API_KEY")
    if not api_key:
        raise ValueError("API key must be provided via --api-key or OPENAI_API_KEY env var")
    
    # Run evaluation
    evaluate_chair_m(
        jsonl_path=args.jsonl_path,
        output_csv=args.output_csv,
        api_key=api_key,
        model=args.model,
        base_url=args.base_url,
        mask_type=args.mask_type,
    )


if __name__ == "__main__":
    main()

